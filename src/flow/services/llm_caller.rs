use crate::error::{Result, AgentFlowError};
use crate::llm::DynLlmClient;
use crate::LlmRequest;
use super::message_parser::MessageParser;
use super::prompt_builder::PromptBuilder;
use super::image_processor::{ImageProcessor, ImageInfo};
use crate::flow::config::{AgentConfig, PromptBuildingRules, FieldExtractionRules};
use crate::flow::constants::{fields, llm as llm_consts};
use serde_json::Value;
use crate::agent::AgentMessage;
use futures::StreamExt;
use anyhow::anyhow;
use std::io::{self, Write};

/// LLM è°ƒç”¨æœåŠ¡
/// 
/// è´Ÿè´£å¤„ç† LLM è°ƒç”¨é€»è¾‘ï¼ŒåŒ…æ‹¬ prompt æ„å»ºã€è¯·æ±‚å‘é€å’Œæµå¼å“åº”å¤„ç†
pub struct LlmCaller;

impl LlmCaller {
    /// è°ƒç”¨ LLM è·å–å“åº”
    /// 
    /// å¦‚æœæä¾›äº† LLM å®¢æˆ·ç«¯ï¼Œåˆ™è°ƒç”¨ LLMï¼›å¦åˆ™ä» payload ä¸­æå– raw å­—æ®µ
    pub async fn call_llm_or_get_raw(
        llm_client: Option<&DynLlmClient>,
        payload: &Value,
        history: &[AgentMessage],
        image_info: &ImageInfo,
        image_base64_final: Option<String>,
        profile: &AgentConfig,
        field_extraction_rules: Option<&FieldExtractionRules>,
        prompt_building_rules: Option<&PromptBuildingRules>,
    ) -> Result<String> {
        if let Some(llm_client) = llm_client {
            Self::call_llm(
                llm_client,
                payload,
                history,
                image_info,
                image_base64_final,
                profile,
                field_extraction_rules,
                prompt_building_rules,
            ).await
        } else {
            Self::get_raw_from_payload(payload)
        }
    }
    
    /// è°ƒç”¨ LLM
    async fn call_llm(
        llm_client: &DynLlmClient,
        payload: &Value,
        history: &[AgentMessage],
        image_info: &ImageInfo,
        image_base64_final: Option<String>,
        profile: &AgentConfig,
        field_extraction_rules: Option<&FieldExtractionRules>,
        prompt_building_rules: Option<&PromptBuildingRules>,
    ) -> Result<String> {
        // æå–ç”¨æˆ·è¾“å…¥
        let user_input_fields: Option<Vec<String>> = field_extraction_rules
            .map(|r| r.user_input_fields.clone());
        let user_input = MessageParser::extract_user_input(
            payload,
            history,
            user_input_fields.as_deref(),
        )?;
        
        // æ„å»ºç³»ç»Ÿ promptï¼ˆåŒ…å«è·¯ç”±æç¤ºï¼‰
        let system_prompt = PromptBuilder::build_system_prompt_with_routing(
            profile.role.as_deref(),
            profile.prompt.as_deref(),
            profile.route_mode.as_deref(),
            profile.route_targets.as_deref(),
            profile.route_prompt.as_deref(),
            prompt_building_rules,
        )?;

        let temperature = prompt_building_rules
            .map(|r| r.temperature)
            .or(profile.temperature)
            .unwrap_or(llm_consts::DEFAULT_TEMPERATURE);

        let llm_request = LlmRequest {
            system: Some(system_prompt),
            user: user_input.to_string(),
            temperature,
            metadata: None,
            image_url: image_info.url.clone(),
            image_base64: image_base64_final,
        };

        let role_name = profile.role.as_deref().unwrap_or(&profile.name);
        
        // ä½¿ç”¨ eprintln ç¡®ä¿ç«‹å³è¾“å‡ºï¼Œä¸å—ç¼“å†²å½±å“
        eprintln!("\n[{}] â³ æ­£åœ¨è°ƒç”¨ LLMï¼Œç­‰å¾…å“åº”...", role_name);
        io::stderr().flush().ok();
        
        // ç¡®ä¿ç«‹å³åˆ·æ–° stdout/stderr
        io::stdout().flush().ok();
        io::stderr().flush().ok();
        
        // è¾“å‡º Agent åç§°å’Œå¼€å§‹æ ‡è®°
        println!("\n[{}] ğŸ“ å“åº”å†…å®¹:", role_name);
        io::stdout().flush().ok();
        
        // è¾“å‡ºç¼©è¿›ï¼Œç”¨äºæµå¼å†…å®¹
        print!("  ");
        io::stdout().flush().ok();
        
        let mut stream = llm_client.complete_stream(llm_request);
        let mut full_response = String::new();
        let mut has_output = false;
        
        while let Some(chunk_result) = stream.next().await {
            match chunk_result {
                Ok(chunk) => {
                    if !chunk.content.is_empty() {
                        // ç›´æ¥è¾“å‡ºåˆ° stdoutï¼Œç«‹å³åˆ·æ–°
                        print!("{}", chunk.content);
                        io::stdout().flush()
                            .map_err(|e| AgentFlowError::Other(anyhow::anyhow!("Failed to flush stdout: {}", e)))?;
                        full_response.push_str(&chunk.content);
                        has_output = true;
                    }
                    if chunk.done {
                        if has_output {
                            println!();
                            io::stdout().flush().ok();
                        }
                        eprintln!("[{}] âœ… å“åº”å®Œæˆ", role_name);
                        io::stderr().flush().ok();
                        break;
                    }
                }
                Err(e) => {
                    eprintln!("\n[{}] âŒ æµå¼è¾“å‡ºé”™è¯¯: {}", role_name, e);
                    io::stderr().flush().ok();
                    return Err(e);
                }
            }
        }
        
        if !has_output {
            eprintln!("[{}] âš ï¸  è­¦å‘Š: æ²¡æœ‰æ¥æ”¶åˆ°ä»»ä½•å“åº”å†…å®¹", role_name);
            io::stderr().flush().ok();
        }
        
        // æœ€åå†æ¬¡ç¡®ä¿æ‰€æœ‰è¾“å‡ºéƒ½è¢«åˆ·æ–°
        io::stdout().flush().ok();
        io::stderr().flush().ok();
        
        Ok(full_response)
    }
    
    /// ä» payload ä¸­è·å– raw å­—æ®µ
    pub fn get_raw_from_payload(payload: &Value) -> Result<String> {
        payload
            .get(fields::RAW)
            .and_then(|v| v.as_str())
            .map(|s| s.to_string())
            .ok_or_else(|| AgentFlowError::Other(anyhow::anyhow!("Missing raw field and no LLM client")))
    }
}

